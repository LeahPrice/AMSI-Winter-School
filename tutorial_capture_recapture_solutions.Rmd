---
title: "Capture-Recapture Example (Tutorial)"
author: "Leah South"
output: 
  pdf_document: default
  html_document: 
    keep_md: yes
params:
  SOLUTION: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(MASS) # multivariate normal
library(coda) # assessing convergence and sample quality
library(psych) # bivariate plots
library(mcmcse) # Multivariate ESS
```


Our running example is about the capture and recapture of the bird species called the European Dipper (\textit{Cinclus cinclus}). Marzolin (1988) collected the data based on the capture and recapture of this species over six years. 

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.85\textwidth]{dipper.jpg}
\end{figure} 

Below are the R packages we'll be using in this document.

```{r, echo = TRUE}
library(MASS) # multivariate normal
library(coda) # assessing convergence and sample quality
library(psych) # bivariate plots
```

# The MCMC Algorithm

We'll be looking at MALA (asymptotically un-baised) and ULA (biased) approaches for inference on our example. The focus is on post-processing and bias rather than implementation of algorithms. You can find the relevant code for MALA and ULA implementations for reference. 

```{r, echo=TRUE}
source('MCMC_fn.R')
source('ULA_fn.R')
```

# The Statistical Model

The parameters for the model are $\phi_i$ and $p_k$ where $i=1,\ldots,6$ and $k=2,\ldots,7$. $\phi_i$ represents the probability of survival from year $i$ to year $i+1$ and $p_k$ represents the probability of being captured in year $k$.

The likelihood for the model is given below, and based on data $D_i$ for the number of birds released in year $i$ and $y_{ik}$ for the number of animals caught in year $k$ out of the number released in year $i$. Here $d_i=D_i-\sum_{k=i+1}^{7}y_{ik}$ is the number released in year $i$ that are never caught. The corresponding probability of a bird being released in year $i$ and never being caught is $\chi_i=1-\sum_{k=i+1}^{7} \phi_i p_k \prod_{m=i+1}^{k-1} \phi_m (1-p_m)$, which is a function of the model parameters. The likelihood is given by

\begin{equation*}
f({y}|\mathbf{{\theta}}) \propto \prod_{i=1}^{6}\chi_i^{d_i} \prod_{k=i+1}^{7} \left[ \phi_i p_k \prod_{m=i+1}^{k-1} \phi_m (1-p_m) \right]^{y_{ik}},
\end{equation*}

where ${\theta}=({\phi},{p})$, ${\phi}=(\phi_1,...,\phi_6)$, ${p}=(p_2,...,p_7)$ and ${y}=\{y_{ik}:i=1,\ldots,6,k=2,\ldots,7\}$. Due to parameter identifiability issues, the parameters $\phi_6$ and $p_7$ are combined as $\phi_6p_7$ leading to a total of eleven parameters.

The prior for each component of ${\theta}$ is set to be $\mathcal{U}(0,1)$, and all components are independent \emph{a priori}. For the RW proposal, the $j$-th parameter ${\theta}[j]$ is transformed using $\tilde{{\theta}}[j]=\log({\theta}[j]/(1-{\theta}[j]))$ for $j=1,\ldots,11$. The implied prior density for $\tilde{{\theta}}[j]$ is then $e^{\tilde{{\theta}}[j]}/(1+e^{\tilde{{\theta}}[j]})^2$, for $j=1,\ldots,11$.

Read in the liklihood functions and tuning parameters for algorithms

```{r model, eval = TRUE, echo = TRUE}
load("recapture_ULA_bettertuning.RData")
load("recapture_MALA_tuning.RData")

# Names of the 11 variables
varNames <- paste0("theta",1:11) #### FIX-LATER

```

\clearpage

\clearpage

# Multiple Chains

## Getting the samples

Let's run 10 chains with a common starting point

```{r BadMulti, eval = TRUE, echo = TRUE, cache=TRUE}
initial <- c(0.35, -0.66, -1.74, 2.5, -0.67, -0.59, 2.38, 2.52, 1.2, 5.08, 
1.3)
set.seed(2)
n_reps <- 10 # number of chains
its <- 500 # number of MCMC iterations
chains_ULA <- chains_MALA <- samples_ULA <- samples_MALA <- list()
for (i in 1:n_reps){
  
  # Running MALA
  single_chain_mala <- MALA_fn(d = 11, initial = initial, covmala = cov_rw, h = h_mala,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
  chains_MALA[[i]] <- single_chain_mala
  samples_MALA[[i]] <- single_chain_mala$samples
  
  # Running ULA
  single_chain_ula <- ULA_fn(d = 11, initial = initial, cov_ULA = cov_ula, h = h_ula,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
  chains_ULA[[i]] <- single_chain_ula
  samples_ULA[[i]] <- single_chain_ula$samples
  
}
ula_noburnin <- as.mcmc.list(samples_ULA)
mala_noburnin <- as.mcmc.list(samples_MALA)
save(ula_noburnin,mala_noburnin,chains_MALA,chains_ULA, file = "ten_chains.RData") # For future reuse
```

```{r}
n_reps <- 10 # number of chains
load("ten_chains.RData")
```

## Compare the KSD

Sourcing in the KSD code and load the KSD package

```{r, echo = T}
source('KSD.R')
library(KSD)
```

Evaluate the KSD on each of the chains for MALA and ULA.

```{r, echo = T, eval=TRUE, cache=TRUE}
KSD_MALA_gaussian <- KSD_ULA_gaussian <- rep(NaN,n_reps)
KSD_MALA_imq <- KSD_ULA_imq <- rep(NaN,n_reps)
for (i in 1:n_reps){
    samples <- as.matrix(chains_MALA[[i]]$samples)
  gradients <- chains_MALA[[i]]$der_loglike + chains_MALA[[i]]$der_logprior
  
  KSD_MALA_gaussian[i] <- KSD(samples, gradients)$ksd
  KSD_MALA_imq[i] <- imqKSD(samples, gradients)
  
  samples <- as.matrix(chains_ULA[[i]]$samples)
  gradients <- chains_ULA[[i]]$der_loglike + chains_ULA[[i]]$der_logprior
  
  KSD_ULA_gaussian[i] <- KSD(samples, gradients)$ksd
  KSD_ULA_imq[i] <- imqKSD(samples, gradients)
}
save(KSD_MALA_gaussian,KSD_ULA_gaussian,KSD_MALA_imq,KSD_ULA_imq, file = "ksd_ten_chains.RData")
boxplot(KSD_MALA_gaussian,KSD_ULA_gaussian)
boxplot(KSD_MALA_imq,KSD_ULA_imq)
```

## Estimating expectations with control variates

Now we'll estimate the posterior expectation of our parameters. The parameters are transformed using $\tilde{{\theta}}[j]=\log({\theta}[j]/(1-{\theta}[j]))$ for $j=1,\ldots,11$. To transform back $\tilde{{\theta}}[j]$ we use $e^{\tilde{{\theta}}[j]}/(1+e^{\tilde{{\theta}}[j]})^2$, for $j=1,\ldots,11$.

```{r CV, eval = TRUE, echo = TRUE}
library(ZVCV)
Vanilla_MALA <- ZV1_MALA <- CF_MALA <- SECF_MALA <- matrix(NaN,nrow=n_reps,ncol=d)
Vanilla_ULA <- ZV1_ULA <- CF_ULA <- SECF_ULA <- matrix(NaN,nrow=n_reps,ncol=d)
for (i in 1:n_reps){
  samples <- as.matrix(chains_MALA[[i]]$samples)
  gradients <- chains_MALA[[i]]$der_loglike + chains_MALA[[i]]$der_logprior
  integrand <- 1/(1+exp(-samples))
  
  # In order: vanilla estimate, zero-variance control variates
  # with a first order polynomila, control functionals and
  # semi-exact control functionals with a first order polynomial
  Vanilla_MALA[i,] <- colMeans(integrand)
  ZV1_MALA[i,] <- zvcv(integrand, samples, gradients,
                       options = list(polyorder = 1, regul_reg = FALSE))$expectation
  CF_MALA[i,] <- CF_crossval(integrand, samples, gradients, kernel_function = "RQ", 
                             sigma_list = list(0.001,0.01,0.1,1,10), folds = 2)$expectation
  SECF_MALA[i,] <- SECF_crossval(integrand, samples, gradients, polyorder = 1, kernel_function = "RQ", 
                                 sigma_list = list(0.001,0.01,0.1,1,10), folds = 2)$expectation
  
  # In order: vanilla estimate, zero-variance control variates
  # with a first order polynomila, control functionals and
  # semi-exact control functionals with a first order polynomial
  samples <- as.matrix(chains_ULA[[i]]$samples)
  gradients <- chains_ULA[[i]]$der_loglike + chains_ULA[[i]]$der_logprior
  integrand <- 1/(1+exp(-samples))

  Vanilla_ULA[i,] <- colMeans(integrand)
  ZV1_ULA[i,] <- zvcv(integrand, samples, gradients, 
                      options = list(polyorder = 1, regul_reg = FALSE))$expectation
  CF_ULA[i,] <- CF_crossval(integrand, samples, gradients, kernel_function = "RQ",
                            sigma_list = list(0.001,0.01,0.1,1,10), folds = 2)$expectation
  SECF_ULA[i,] <- SECF_crossval(integrand, samples, gradients, polyorder = 1, kernel_function = "RQ",
                                sigma_list = list(0.001,0.01,0.1,1,10), folds = 2)$expectation
}
load("Recapture_goldstandard.RData")
# Boxplots of the estimates
for (j in 1:11){
  boxplot(Vanilla_MALA[,j],ZV1_MALA[,j],CF_MALA[,j],SECF_MALA[,j],
          Vanilla_ULA[,j],ZV1_ULA[,j],CF_ULA[,j],SECF_ULA[,j])
  abline(h=gold_standard[j])
}
```

## Investigate tuning parameter for ULA

Try increasing and decreasing the parameter h_ula.

```{r ULAtry, eval = TRUE, echo = TRUE, cache=TRUE}
set.seed(2)
h_value <- 0.5
# Running ULA
single_chain_ula <- ULA_fn(d = 11, initial = initial, cov_ULA = cov_ula, h = h_value,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
samples <- as.matrix(single_chain_ula$samples)
gradients <- single_chain_ula$der_loglike + single_chain_ula$der_logprior

KSD(samples, gradients)$ksd
imqKSD(samples, gradients)

h_value <- 1
# Running ULA
single_chain_ula <- ULA_fn(d = 11, initial = initial, cov_ULA = cov_ula, h = h_value,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
samples <- as.matrix(single_chain_ula$samples)
gradients <- single_chain_ula$der_loglike + single_chain_ula$der_logprior

KSD(samples, gradients)$ksd
imqKSD(samples, gradients)


h_value <- 1.6
# Running ULA
single_chain_ula <- ULA_fn(d = 11, initial = initial, cov_ULA = cov_ula, h = h_value,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
samples <- as.matrix(single_chain_ula$samples)
gradients <- single_chain_ula$der_loglike + single_chain_ula$der_logprior

KSD(samples, gradients)$ksd
imqKSD(samples, gradients)

```

## Investigate convergence for MALA

Try alternative initialisation points you may use the following to simulate from the prior:

```{r,echo=TRUE}
initial <- recapture_simprior(1, options)
```

```{r repeated, eval = TRUE, echo = TRUE, cache=TRUE}
set.seed(2)
n_reps <- 10 # number of chains
its <- 500 # number of MCMC iterations
chains_ULA <- chains_MALA <- samples_ULA <-
  samples_MALA <- list()
for (i in 1:n_reps){
  initial <- recapture_simprior(1, options)
  # Running MALA
  single_chain_mala <- MALA_fn(d = 11, initial = initial, covmala = cov_rw, h = h_mala,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
  chains_MALA[[i]] <- single_chain_mala
  samples_MALA[[i]] <- single_chain_mala$samples
  
  # Running ULA
  single_chain_ula <- ULA_fn(d = 11, initial = initial, cov_ULA = cov_ula, h = h_ula,
                            iters = its,der_loglike = der_loglike, 
                            der_logprior = der_logprior, 
                            options = options, varNames = varNames)
  chains_ULA[[i]] <- single_chain_ula
  samples_ULA[[i]] <- single_chain_ula$samples
  
}
ula_noburnin <- as.mcmc.list(samples_ULA)
mala_noburnin <- as.mcmc.list(samples_MALA)
```



Investigate the convergence using multiple chains and Gelman & Rubin's $\hat{R}$ diagnostic. Work out a good burnin and investigate control variates on the burnin chain.

```{r MultiTrace, eval = TRUE, echo = TRUE, cache = FALSE}
plot(mala_noburnin,smooth=FALSE)
```


## Rhat

The $\hat{R}$ diagnostic is not $< 1.1$ for all dimensions, so we have evidence of non-convergence.

```{r Rhat, eval = TRUE, echo = TRUE, cache = FALSE}
gelman.diag(mala_noburnin,autoburnin=FALSE)
```


## Removing Burn-In

Let's see if we still have evidence of non-convergence if we remove the first 500 samples as burn-in.

```{r MultiBurnin, eval = TRUE, echo = TRUE, cache = TRUE}
# Removing burn-in
burnin <- 300
its <- 500
mala_burnin <- list()
for (i in 1:n_reps){
  mala_burnin[[i]] <- mcmc(mala_noburnin[[i]][(burnin+1):(its),])
}
mala_burnin <- as.mcmc.list(mala_burnin)

# Plotting chains
plot(mala_burnin,smooth=FALSE)

# Calculating Rhat
gelman.diag(mala_burnin,autoburnin=FALSE)
```

Investigate the KSD without bias:

```{r, echo = T, eval=TRUE, cache=TRUE}
burnin <- 300
KSD_MALA_gaussian <- KSD_ULA_gaussian <- rep(NaN,n_reps)
KSD_MALA_imq <- KSD_ULA_imq <- rep(NaN,n_reps)
for (i in 1:n_reps){
  samples <- as.matrix(chains_MALA[[i]]$samples[(burnin+1):(its),])
  gradients <- chains_MALA[[i]]$der_loglike[(burnin+1):(its),] + chains_MALA[[i]]$der_logprior[(burnin+1):(its),]
  
  KSD_MALA_gaussian[i] <- KSD(samples, gradients)$ksd
  KSD_MALA_imq[i] <- imqKSD(samples, gradients)
  
  samples <- as.matrix(chains_ULA[[i]]$samples[(burnin+1):(its),])
  gradients <- chains_ULA[[i]]$der_loglike[(burnin+1):(its),] + chains_ULA[[i]]$der_logprior[(burnin+1):(its),]
  
  KSD_ULA_gaussian[i] <- KSD(samples, gradients)$ksd
  KSD_ULA_imq[i] <- imqKSD(samples, gradients)
}
boxplot(KSD_MALA_gaussian,KSD_ULA_gaussian)
boxplot(KSD_MALA_imq,KSD_ULA_imq)
```



